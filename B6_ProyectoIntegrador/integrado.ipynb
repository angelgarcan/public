{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Multiple outputs per cell in Jupyter \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%load_ext jupyternotify\n",
    "%autonotify -a 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "// Evitar autoscroll.\n",
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "// Evitar autoscroll.\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from integrado import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Processing data/geo-mx-2004_min.json ...\n",
      "2020-05-12 20:54:48.410406 :: 0.000294 - Processing item #0 \n",
      "** Processed 1000 lines. Saved to /home/jovyan/public/B6_ProyectoIntegrador/data/geo-mx-2004_min.npy.\n"
     ]
    }
   ],
   "source": [
    "tokens=Tokens(\"data/geo-mx-2004_min.json\",replace=True,showProgressEach=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_=Index(tokens,showProgressEach=10000)\n",
    "len(index_.postlists.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds=index_.o(\"covid coronavirus pandemia cuarentena\",tokenize=True)\n",
    "print(len(ds),ds[:5])\n",
    "docs=tokens.getToks(ds)\n",
    "print(len(docs),docs[:5])\n",
    "[tw['text'] for tw in tokens.getTws(ds[:5])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings():\n",
    "    \n",
    "    def __init__(self, tokens, showProgressEach=1000):\n",
    "        print(\"Init...\")\n",
    "        model = FastText(size=100, window=3, min_count=1, min_n=2, max_n=5)\n",
    "        print(\"Build...\")\n",
    "        model.build_vocab(sentences=tokens)\n",
    "        print(\"Train...\")\n",
    "        model.train(sentences=tokens,total_examples=len(tokens), epochs=10)\n",
    "        print(\"Mean...\")        \n",
    "        arry=[]\n",
    "        tx = datetime.datetime.now()\n",
    "        for idx, tw in enumerate(tokens):\n",
    "            tb.show_progress(showProgressEach,tx,idx)\n",
    "            arry.append(np.mean([model.wv[tok] for tok in tw],axis=0))\n",
    "        self.embeddings_=np.array(arry)\n",
    "#         self.embeddings_=np.array(\n",
    "#             [np.mean([model.wv[tok] for tok in tw],axis=0) \n",
    "#                    for tw in tokens])\n",
    "        del(model)\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.N\n",
    "type(tokens.getToks(range(tokens.N))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init...\n",
      "Build...\n",
      "Train...\n",
      "Mean...\n",
      "2020-05-12 21:37:41.825642 :: 1.1e-05 - Processing item #0 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_list=tokens.getToks(range(tokens.N))\n",
    "embs=Embeddings(tokens_list)\n",
    "del(tokens_list)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (1000, 100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.03046205, -0.25796747,  0.13576251, ...,  0.1594239 ,\n",
       "         0.02915039, -0.06679923],\n",
       "       [-0.0370128 , -0.32056278,  0.16785839, ...,  0.19623241,\n",
       "         0.03758134, -0.08285397],\n",
       "       [-0.04223422, -0.36701065,  0.19386688, ...,  0.22578895,\n",
       "         0.04409511, -0.09442057],\n",
       "       ...,\n",
       "       [-0.03174646, -0.27930507,  0.14676622, ...,  0.1712946 ,\n",
       "         0.03309873, -0.07166143],\n",
       "       [-0.04018831, -0.33646196,  0.17742077, ...,  0.20634827,\n",
       "         0.03972435, -0.0861236 ],\n",
       "       [-0.03055495, -0.27392912,  0.14385511, ...,  0.16906819,\n",
       "         0.03253127, -0.07125088]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(embs.embeddings_), embs.embeddings_.shape)\n",
    "embs.embeddings_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skeleton import Clustering\n",
    "class Clustering2(Clustering):\n",
    "    \n",
    "    # se asigna el método de inicialización de Kmeans (default=\"fft\")\n",
    "    def __init__(self, *args, k_init=\"fft\", **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.k_init_ = k_init\n",
    "        \n",
    "    # asigna los elementos en la colección a su centroide más cercano\n",
    "    # genera las etiquetas de los clusters\n",
    "    # almacena la distancia de cada elemento a su centroide más cercano\n",
    "    def _assign_nearest_centroids(self):\n",
    "        self.labels_=[-1 for _ in self.data]\n",
    "        self.dists_=[-1 for _ in self.data]\n",
    "        for j in range(len(self.data)):\n",
    "            dists = [(self.distance_function(c, self.data[j]), i, self.data[j])\n",
    "                     for i, c in self.centroids_.items()]\n",
    "            dists.sort()\n",
    "            self.labels_[j]=dists[0][1]\n",
    "            self.dists_[j]=(dists[0][0],j)\n",
    "            \n",
    "    def KMeans(self):\n",
    "        self.centroids_ = {}\n",
    "        C = []\n",
    "        if self.k_init_ == 'fft':\n",
    "            self.FFTraversal()\n",
    "            for i, c in self.centroids_.items():\n",
    "                C.append(c)\n",
    "        elif self.k_init_ == 'random':\n",
    "            C = np.random.uniform(low=np.min(self.data, axis=0) * .4,\n",
    "                                  high=np.max(self.data, axis=0) * .4,\n",
    "                                  size=(self.n_clusters, self.data.shape[1]))\n",
    "#             self.labels_ = []\n",
    "#             for i in range(len(self.data)):\n",
    "#                 self.labels_.append(np.int64(np.random.choice(range(self.n_clusters))))\n",
    "#             for i in range(self.n_clusters):\n",
    "#                 cs = self.data[np.where(self.labels_ == np.int64(i))[0]]\n",
    "#                 if len(cs) > 0:\n",
    "#                     C.append(np.mean(cs, axis=0))\n",
    "        else:\n",
    "            raise Exception(f\"k_init={self.k_init_} is not defined\")\n",
    "            \n",
    "        C = np.array(C)\n",
    "        error = np.inf\n",
    "        while error != 0:\n",
    "            self.labels_ = []\n",
    "            C_old = deepcopy(C)\n",
    "            for i in range(len(self.data)):\n",
    "                distances = [(self.distance_function(c, self.data[i]), j, self.data[i])\n",
    "                    for j, c in enumerate(C)]\n",
    "                distances.sort()\n",
    "                cluster = distances[0][1]\n",
    "                self.labels_.append(cluster)\n",
    "            for i in range(self.n_clusters):\n",
    "                cs = self.data[np.where(self.labels_ == np.int64(i))[0]]\n",
    "                if len(cs) > 0:\n",
    "                    C[i] = np.mean(cs, axis=0)\n",
    "            error = np.sum(C - C_old)\n",
    "        self.centroids_ = {}\n",
    "        for i, c in enumerate(C):\n",
    "            self.centroids_[np.int32(i)] = c\n",
    "        self._inertia()\n",
    "        return self\n",
    "\n",
    "    def FFTraversal(self):\n",
    "        self.centroids_ = {}\n",
    "        c1_idx = np.random.choice(self.data.shape[0])\n",
    "        self.centroids_[np.int32(c1_idx)] = self.data[c1_idx, :]\n",
    "        while len(self.centroids_.keys()) < self.n_clusters:\n",
    "            self._assign_nearest_centroids()\n",
    "            self.dists_.sort()\n",
    "            cn_idx=self.dists_[-1][1]\n",
    "            self.centroids_[np.int32(cn_idx)] = self.data[cn_idx, :]\n",
    "        self._assign_nearest_centroids()\n",
    "#         self.dists_.sort()\n",
    "        self._inertia()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "import sys\n",
    "local_vars = list(locals().items())\n",
    "tot=0\n",
    "vars_=[]\n",
    "for var, obj in local_vars:\n",
    "    mem=sys.getsizeof(obj)\n",
    "    tot+=mem\n",
    "    vars_.append((mem,var))\n",
    "print(\"Total =\",tot)\n",
    "vars_.sort(reverse=True)\n",
    "print(vars_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
