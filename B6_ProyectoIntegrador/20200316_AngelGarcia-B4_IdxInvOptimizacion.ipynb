{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bloque 3. Actividad 2. Búsqueda (continuación)\n",
    "\n",
    "|||\n",
    "|-|-|\n",
    "|**Alumno**|Ángel García Alcántara<br><angelgarcan@gmail.com>|\n",
    "|**Materia**|Recuperación de información<br>en Bases de datos no estructuradas<br>Cuarto Semestre|\n",
    "|**Fecha**|16 de marzo de 2020|\n",
    "\n",
    "Crear un índice invertido con pesado TFIDF, implemente podado de listas de posteo y mida el impacto del podado.\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "* Utilice el archivo de tweets de las elecciones 2018 como fuente de datos, contiene 100 mil tweets (cada linea es un json valido, el archivo esta comprimido con gzip).\n",
    "* Modele su texto como una bolsa de palabras (realice las normalizaciones que juzgue necesarias)\n",
    "    * Cree un índice sobre el campo \"text\"\n",
    "    * Construya el índice invertido\n",
    "    * Realice el siguiente experimento\n",
    "        * Seleccione de manera aleatoria 1000 documentos y uselos como consultas\n",
    "        * Realice la operación de poda de listas de posteo para diferentes longitudes máximas, en particular, pruebe para 50, 100, 150, 200\n",
    "        * Mida el tiempo de consulta de las 1000 consultas, para cada tamaño de poda\n",
    "        * Plotee los histogramas de la similitud coseno, para cada tamaño de poda; trabaje con datos agregados para las 1000 consultas\n",
    "        * Mida la cantidad de memoria después de cada poda (puede guardar la estructura mediante el modulo  pickle  de Python, o si se le facilita, puede realizar el conteo de enteros y flotantes requeridos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INTRODUCCIÓN\n",
    "\n",
    "### Índice invertido\n",
    "En informática, un índice invertido es un índice de base de datos que almacena una asignación del contenido, como palabras o números, a sus ubicaciones en una tabla, o en un documento o un conjunto de documentos. El propósito de un índice invertido es permitir búsquedas rápidas de texto completo, a un costo de mayor procesamiento cuando se agrega un documento a la base de datos. Es la estructura de datos más popular utilizada en los sistemas de recuperación de documentos. Además, varios sistemas importantes de administración de bases de datos basados en mainframes de uso general han utilizado arquitecturas de listas invertidas.\n",
    "\n",
    "Hay dos variantes principales de índices invertidos: un índice invertido a nivel de registro contiene una lista de referencias a documentos para cada palabra. Un índice invertido a nivel de palabra contiene adicionalmente las posiciones de cada palabra dentro de un documento. La última forma ofrece más funcionalidad (como búsquedas de frases) pero necesita más potencia de procesamiento y espacio para ser creada.\n",
    "\n",
    "### Boolean Retrieval\n",
    "El modelo booleano (estándar) de recuperación de información (BIR) es un modelo clásico de recuperación de información y, al mismo tiempo, el primero y más adoptado. Es utilizado por muchos sistemas de IR hasta el día de hoy. El BIR se basa en la lógica booleana y la teoría clásica de conjuntos, ya que tanto los documentos a buscar como la consulta del usuario se conciben como conjuntos de términos. La recuperación se basa en si los documentos contienen o no los términos de la consulta.\n",
    "\n",
    "Ventajas:\n",
    "* Formalismo limpio\n",
    "* Fácil de implementar\n",
    "* Concepto intuitivo\n",
    "\n",
    "Desventajas\n",
    "* La coincidencia exacta puede recuperar muy pocos o demasiados documentos\n",
    "* Difícil traducir una consulta a una expresión booleana\n",
    "* Todos los términos están igualmente ponderados\n",
    "* Más como recuperación de datos que recuperación de información\n",
    "\n",
    "### TF-IDF\n",
    "TF-IDF significa \"Frecuencia de término - Frecuencia de documento inversa\". Esta es una técnica para cuantificar una palabra en documentos, generalmente calculamos un peso para cada palabra que significa la importancia de la palabra en el documento y el corpus. Este método es una técnica ampliamente utilizada en recuperación de información y minería de texto.\n",
    "\n",
    "\\begin{equation}\n",
    "TF-IDF = Term Frequency (TF) * Inverse Document Frequency (IDF)\n",
    "\\end{equation}\n",
    "\n",
    "Al tomar las fórmulas de TF e IDF, obtenemos la puntuación TF-IDF:\n",
    "\n",
    "\\begin{equation}\n",
    "\\textit{tf-idf(x)}=\\textit{tf(t,d)} * \\log(\\frac{N}{df+1})\n",
    "\\end{equation}\n",
    "\n",
    "En nuestro caso al usar `TfidfVectorizer` se calcula IDF y TF de la siguiente manera (de la documentación de `sklearn.feature_extraction.text.TfidfVectorizer`):\n",
    "\n",
    "> La fórmula que se usa para calcular el tf-idf para un término t de un documento d en un conjunto de documentos es tf-idf (t, d) = tf (t, d) * idf (t), y se calcula el idf como idf (t) = log [n / df (t)] + 1 (si smooth_idf = False), donde n es el número total de documentos en el conjunto de documentos y df (t) es la frecuencia de documentos de t; la frecuencia del documento es el número de documentos en el conjunto de documentos que contienen el término t. El efecto de agregar \"1\" a la idf en la ecuación anterior es que los términos con idf cero, es decir, los términos que aparecen en todos los documentos en un conjunto de entrenamiento, no serán completamente ignorados. (Tenga en cuenta que la fórmula de idf anterior difiere de la notación de libro de texto estándar que define el idf como idf (t) = log [n / (df (t) + 1)]).\n",
    "\n",
    "\n",
    "### TF. Term Frequency\n",
    "Esto mide la frecuencia de una palabra en un documento. Esto depende en gran medida de la longitud del documento y de la generalidad de la palabra, por ejemplo, una palabra muy común como \"es\" puede aparecer varias veces en un documento. Pero si tomamos dos documentos, uno con 100 palabras y otro con 10,000 palabras, hay una alta probabilidad de que la palabra común como \"es\" pueda estar presente más en el documento redactado de 10,000. Pero no podemos decir que el documento más largo es más importante que el documento más corto. Por esta razón exacta, realizamos la normalización en el valor de frecuencia. Dividimos la frecuencia con el número total de palabras en el documento.\n",
    "\n",
    "\\begin{equation}\n",
    "\\textit{tf(t,d)} = \\frac {\\textit(count\\ of\\ t\\ in\\ d)} {\\textit(number\\ of\\ words\\ in\\ d)}\n",
    "\\end{equation}\n",
    "\n",
    "### IDF. Inverse Document Frequency\n",
    "DF mide la importancia del documento en todo el conjunto de corpus, esto es muy similar al TF. La única diferencia es que TF es un contador de frecuencia para un término t en el documento d, donde como DF es el recuento de ocurrencias del término t en el conjunto de documentos N. En otras palabras, DF es el número de documentos en los que la palabra está presente. Consideramos una ocurrencia si el término consiste en el documento al menos una vez, no necesitamos saber la cantidad de veces que el término está presente.\n",
    "\n",
    "$$df(t) = occurrence\\ of\\ t\\ in\\ documents$$\n",
    "\n",
    "IDF es el inverso de la frecuencia del documento que mide la relevancia del término t. Cuando calculemos IDF, será muy bajo para las palabras más frecuentes, como las palabras de detención (porque las palabras de detención como \"es\" están presentes en casi todos los documentos, y N / df dará un valor muy bajo a esa palabra). Esto finalmente da lo que queremos, un peso relativo.\n",
    "\n",
    "$$idf(t) = \\log{\\frac{N}{(df + 1)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DESARROLLO\n",
    "Se a desarrollar el código necesario para realizar el trabajo solicitado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple outputs per cell in Jupyter \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ocultar In[] y Out[] en cada celda.\n",
    "# from IPython.core.display import display,HTML\n",
    "# display(HTML('<style>.prompt{width: 0px; min-width: 0px; visibility: collapse}</style>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "// Evitar autoscroll.\n",
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "// Evitar autoscroll.\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'microtc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-033ab53c50b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmicrotc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtweet_iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'microtc'"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import datetime\n",
    "import random\n",
    "import re\n",
    "from microtc.utils import tweet_iterator\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm,trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones misceláneas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# MISC\n",
    "#################\n",
    "\n",
    "# Imprime el progreso dentro de un ciclo for.\n",
    "def show_progress(showProgressEach, tx, idx, legend='', ret_string=False):\n",
    "    if showProgressEach is not None and idx % showProgressEach == 0:\n",
    "        tx1 = datetime.datetime.now()\n",
    "        dx = (tx1 - tx).total_seconds()\n",
    "        label=f\"{tx1} :: {dx} - Processing item #{idx} {legend}\"\n",
    "        if ret_string: \n",
    "            return tx1,label\n",
    "        else:\n",
    "            print(label)\n",
    "            return tx1,None\n",
    "    elif ret_string: return tx,None\n",
    "\n",
    "# Convierte una matriz dispersa en un numpy.array.\n",
    "def mtxvec2array(mtxvec):\n",
    "    return np.squeeze(np.asarray(mtxvec.todense()))\n",
    "\n",
    "# Cuenta el total de elementos no cero en una matriz dispersa triangular.\n",
    "def count_nonzero_triangular(triangular_mtx):\n",
    "    nprocs=triangular_mtx.count_nonzero()\n",
    "    nprocs_diag=np.array([triangular_mtx[i,i] for i in range(triangular_mtx.shape[0])]).nonzero()[0].shape[0]\n",
    "    nprocs-=nprocs_diag\n",
    "    nprocs/=2\n",
    "    nprocs+=nprocs_diag\n",
    "    return int(nprocs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proceso de limpieza de texto\n",
    "Consiste en los siguientes pasos:\n",
    "- Convertir a minúsculas,\n",
    "- Remover saltos de línea,\n",
    "- Remover acentos ,\n",
    "- Reemplazar caracteres raros (ü y ñ),\n",
    "- Remover URL's,\n",
    "- Remover signos de puntuación (también números),\n",
    "- Reemplazar cadenas de carateres repetidos 3 veces o más por solo dos ocurrencias,\n",
    "- Remover palabras de un solo un caracter, ya que no aportan significado real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanStr(line):\n",
    "    # Limpiando texto.\n",
    "    text = line.lower()\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    # Acentos.\n",
    "    text = re.sub(r'á', 'a', text)\n",
    "    text = re.sub(r'é', 'e', text)\n",
    "    text = re.sub(r'í', 'i', text)\n",
    "    text = re.sub(r'ó', 'o', text)\n",
    "    text = re.sub(r'ú', 'u', text)\n",
    "    # Caracteres raros.\n",
    "    text = re.sub(r'ü', 'u', text)\n",
    "    text = re.sub(r'ñ', 'n', text)\n",
    "    text = re.sub(r'http[^ ]+', '', text)  # URLs.\n",
    "#     text = re.sub(r'@[^ ]+', '', text)  # Citas.\n",
    "#     text = re.sub(r'#[^ ]+', '', text)  # Tags.\n",
    "    text = re.sub(r'[^a-z ]', ' ', text).strip()  # Signos de puntuación.\n",
    "    text = re.sub(r'(.)\\1\\1+', r'\\1\\1', text)  # Caracteres repetidos 3 veces o más.\n",
    "    text = re.sub(r' [^ ] ', ' ', text)  # Caracteres huérfanos.\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remoción de stopwords\n",
    "Dado que son palabras que no aportan significado realmente, se eliminan.<br>\n",
    "El idioma es el español."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('spanish')\n",
    "\n",
    "def removeStopWords(txtLst):\n",
    "    new_txtLst = txtLst[:]\n",
    "    idx = 0\n",
    "    while idx < len(new_txtLst):\n",
    "        w = new_txtLst[idx]\n",
    "        if w in stopwords:\n",
    "            new_txtLst.remove(w)\n",
    "        else:\n",
    "            idx += 1\n",
    "    return new_txtLst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "Se reducen las palabras a su raíz común."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "def stemming(txtLst):\n",
    "    new_txtLst = txtLst[:]\n",
    "    for idx, w in enumerate(new_txtLst):\n",
    "        new_txtLst[idx] = stemmer.stem(w)            \n",
    "    return new_txtLst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesar línea de texto\n",
    "Se limpia, remueven stopwords, stemming y se devuelven los tokens en formato set o lista.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpia y tokeniza una línea de texto.\n",
    "def process_line(line, remStopw=True, stemm=True, retSet=False):\n",
    "    # Limpiando text.\n",
    "    twTxtLst = cleanStr(line).split()\n",
    "    if remStopw: twTxtLst = removeStopWords(twTxtLst)\n",
    "    if stemm: twTxtLst = stemming(twTxtLst)\n",
    "    if retSet:\n",
    "        return set(twTxtLst)\n",
    "    else:\n",
    "        return twTxtLst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Índice Invertido\n",
    "\n",
    "##### Algoritmo y consideraciones\n",
    "Una vez estudiado el problema y los conceptos asociados de tomaron las siguientes decisiones de diseño del algoritmo:\n",
    "* <u>Recorrer una sola vez todos los tweets</u>. Dado que consume tiempo de procesamiento considerable se decidió solo hacerlo una vez. Para lograrlo se almacena en memoria y en un archivo `tokenized_docs.pickle` las listas de tokens resultantes de procesar el texto de cada tweet. \n",
    "* <u>No almacenar los 100,000 textos de los tweets</u>. No nos interesa tener disponible el texto per se, solo las frecuencias de los términos relativas a cada tweet(documento) y a todos los documentos.\n",
    "* <u>Computar la matriz de pesos TF-IDF y usarla como fuente de datos</u>. Lo anterior significa que a partir de la matriz dispersa obtenida con `TfidfVectorizer` se obtendrán las listas de posteo y las representaciones BOW de cada documento.\n",
    "* <u>Representar simplificadamente los vectores BOW de cada documento</u>. Para su rápido acceso se representan y almacenan los vectores de documentos como dos arrays, uno con los índices y otro con los valores. Esto se hace así ya que el vector BOW original tiene demasiados ceros lo cual hace ineficiente su almacenamiento y procesamiento. \n",
    "* <u>Normalizar los vectores BOW de los documentos</u>. Se normalizan los vectores documento y se aprovecha este hecho para simplificar el cálculo del producto punto de la fórmula de similitud coseno (véase _\"Similitud Coseno. Procesamiento y visualización\"_ ).  \n",
    "* <u>Agregar el peso TF-IDF a las listas de posteo</u>. Las listas de posteo se representan como tuplas con el índice de documento y el score TF-IDF calculado.\n",
    "\n",
    "A continuación se agrupan programáticamente los métodos relacionados al manejo y procesamiento del índice invertido en la clase `InvertedIdx`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedIdx:\n",
    "    tokenized_docs_filename='tokenized_docs.pickle'\n",
    "    tokenized_docs=None\n",
    "    idx_mtx=None\n",
    "    postlists=None\n",
    "    docs_norms=[]\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "    \n",
    "    def process(self, showProgressEach=None, remStopw=True, stemm=True, replace=False):\n",
    "        \"\"\"Procesa todos los tweets, los tokeniza y almacena en un archivo pickle.\"\"\"\n",
    "        if self.tokenized_docs:            \n",
    "            print(f\"** Processed. tokenized_docs is already loaded.\")\n",
    "            return\n",
    "        \n",
    "        # Si existe el archivo. \n",
    "        if os.path.isfile(self.tokenized_docs_filename) and not replace:\n",
    "            with open(self.tokenized_docs_filename, 'rb') as f:\n",
    "                self.tokenized_docs= pickle.load(f)   \n",
    "            print(f\"** Processed. {os.path.abspath(self.tokenized_docs_filename)} already exists. tokenized_docs loaded.\") \n",
    "            return\n",
    "        \n",
    "        print(f\"** Processing {self.filename} ...\")\n",
    "        self.tokenized_docs=[]\n",
    "        tx = datetime.datetime.now()\n",
    "        for idx, tw in enumerate(tweet_iterator(self.filename)):\n",
    "            show_progress(showProgressEach, tx, idx)\n",
    "            twTokens = process_line(tw['text'], remStopw, stemm)\n",
    "            self.tokenized_docs.append(twTokens)\n",
    "#             if not twTokens:\n",
    "#                 print(idx, twTokens, tw['text'])\n",
    "\n",
    "        self.n_tw = idx + 1\n",
    "        self.docs_range = set(range(1, self.n_tw + 1))\n",
    "        \n",
    "        with open(self.tokenized_docs_filename, 'wb') as f:\n",
    "            pickle.dump(self.tokenized_docs, f)            \n",
    "        print(f\"** Processed {self.n_tw} lines. Saved to {os.path.abspath(self.tokenized_docs_filename)}.\")\n",
    "        \n",
    "    def compute_mtx(self, n_poda=0):\n",
    "        \"\"\"\n",
    "        Genera la matriz self.idx_mtx que contiene los vectores de cada palabra.\n",
    "        Almacena la matriz resultante en un archivo pickle.\n",
    "        \"\"\"\n",
    "        if self.idx_mtx is None:\n",
    "            print(\"** Computing TF-IDF...\")\n",
    "            tfidf_vectorizer=TfidfVectorizer(use_idf=True,\n",
    "                                             norm='l2',\n",
    "                                             analyzer='word',\n",
    "                                             tokenizer=lambda x: x,\n",
    "                                             preprocessor=lambda x: x,\n",
    "                                             token_pattern=None) \n",
    "            self.idx_mtx=sp.lil_matrix(tfidf_vectorizer.fit_transform(self.tokenized_docs).T)\n",
    "            self.corpus=tfidf_vectorizer.get_feature_names()\n",
    "            print(\"** Computed TF-IDF Matrix !!!\")\n",
    "        else:            \n",
    "            print(\"** TF-IDF Matrix was already computed !!!\")\n",
    "        \n",
    "        if n_poda != 0:        \n",
    "            print(f\"** Starting poda{n_poda} TF-IDF Matrix...\")\n",
    "            tx = datetime.datetime.now()\n",
    "            for idx_w,w in enumerate(inv_idx.corpus):\n",
    "                show_progress(1000, tx, idx_w)\n",
    "                vd=np.asarray(self.idx_mtx.getrow(idx_w).todense()).flatten()\n",
    "                cols=vd.argsort()[:-n_poda]\n",
    "                vd[cols]=0\n",
    "                self.idx_mtx[idx_w,:]=vd\n",
    "        \n",
    "        filename=f\"poda{n_poda}_mtx.pickle\"\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(self.idx_mtx, f)\n",
    "        print(f\"** Done poda{n_poda} TF-IDF Matrix !!!\")\n",
    "        print(f\"{filename} written.\")\n",
    "            \n",
    "        return self.idx_mtx\n",
    "\n",
    "    def compute_plists(self,n_poda=0):\n",
    "        \"\"\"\n",
    "        Computa las listas de posteo para el tamaño de poda n_poda.\n",
    "        Guarda las listas de posteo en la variable self.postlists.\n",
    "        Almacena las listas de posteo resultantes en un archivo pickle.\n",
    "        \"\"\"\n",
    "        print(f\"** Computing Posting lists with poda{n_poda} ...\")\n",
    "        self.postlists=defaultdict(list)\n",
    "        tx = datetime.datetime.now()\n",
    "        for idx_w,w in enumerate(inv_idx.corpus):\n",
    "            show_progress(10000, tx, idx_w)\n",
    "            vec=np.asarray(self.idx_mtx.getrow(idx_w).todense()).flatten()\n",
    "            plen=0\n",
    "            for idx_v in reversed(vec.argsort()):\n",
    "                v = vec[idx_v]\n",
    "                if v == 0 : break\n",
    "                self.postlists[w].append((idx_v,v))\n",
    "                plen += 1\n",
    "                if n_poda != 0 and plen == n_poda: break\n",
    "        \n",
    "        filename=f\"poda{n_poda}_plist.pickle\"\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(self.postlists, f)\n",
    "        print(f\"{filename} written.\")\n",
    "        print(f\"** Computed Posting lists with poda{n_poda} !!!\")    \n",
    "        return self.postlists\n",
    "    \n",
    "    def plist_index(self, token):\n",
    "        \"\"\"Obtiene solo los índices en los que ocurre el token dado.\"\"\"\n",
    "        return [idx for idx,score in self.postlists[token]]\n",
    "    \n",
    "    def a(self, q):\n",
    "        \"\"\"Realiza la búsqueda AND de todos los tokens q.\"\"\"\n",
    "        d = self.docs_range\n",
    "        for i, word in enumerate(q):\n",
    "            d = d.intersection(self.plist_index(word))\n",
    "        return list(sorted(d))\n",
    "\n",
    "    def o(self, q):\n",
    "        \"\"\"Realiza la búsqueda OR de todos los tokens q.\"\"\"\n",
    "        d = set()\n",
    "        for word in q:\n",
    "            d = d.union(self.plist_index(word))\n",
    "        return list(sorted(d))\n",
    "\n",
    "    def getTw(self, n):\n",
    "        \"\"\"Obtiene el tweet en la posición n.\"\"\"\n",
    "        for idx, tw in enumerate(tweet_iterator(self.filename)):\n",
    "            if idx == n: return tw;\n",
    "    \n",
    "    def getDocs(self, nTweets):\n",
    "        \"\"\"Obtiene el(los) documento(s) tokenizado del tweet en la posición n.\"\"\"\n",
    "        return [self.tokenized_docs[i] for i in nTweets]\n",
    "\n",
    "    # ONLY FOR DEBUGGING.\n",
    "    def printPostlist(self, u, printText=False):\n",
    "        A = self.sorted_inv_indx[u]\n",
    "        print(u, len(A), A)\n",
    "        if printText:\n",
    "            for idx in A:\n",
    "                print(idx, inv_idx.getTw(idx)['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# !wc e18_min20.json\n",
    "!rm -f *.pickle\n",
    "!ls -l *.pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea el objeto que contendrá el índice invertido y se procesa todo el archivo de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inv_idx = InvertedIdx(\"e18.json\")\n",
    "inv_idx.process(showProgressEach=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procesa la matriz TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "inv_idx.compute_mtx()\n",
    "# pd.DataFrame(inv_idx.idx_mtx.todense(), index=inv_idx.corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesamiento\n",
    "##### Representación BOW simplificada de los documentos.\n",
    "Para hacer más eficiente el cálculo se preprocesa y almacenana en memoria la representación sintétizada de los documentos. Se conforma por el índice y el valor TF-IDF proveniente del vector de BOW (bag of words) del documento. Se almacena en dos listas de arrays `docs_idx` y `docs_val` (1000 vectores en cada caso).\n",
    "\n",
    "Lo anterior mejora en gran medida el cálculo de las similitudes coseno entre documentos necesarias para graficar los histogramas de los datos agregados para las 1000 consultas.\n",
    "\n",
    "Adicionalmente se crea una matriz dispersa `docs_prods` para almacenar los productos punto entre documentos (100,000 * 100,000 - 100,000 = 9,999,900,000 pares). Dada la cantidad de pares existentes no se preprocesan todos, pero sí se almacenan la primera vez que se calculan para evitar calcularlos dos veces. Esto reduce considerablemente el tiempo de procesamiento necesario. \n",
    "\n",
    "Otro motivo para hacerlo de esta manera es el hecho que, por lo que hemos aprendido analizando la relación entre las palabras en un lenguaje dado, la diferencia en la frecuancia de ocurrencia entre los bigramas mas frecuentes y los menos es muy grande. Sabiendo esto se deduce que habrá muchos bigramas que nunca ocurran, por lo que no será necesario calcular su producto punto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_docs=inv_idx.idx_mtx.shape[1]\n",
    "n_pairs=n_docs**2-n_docs\n",
    "docs_prods=sp.lil_matrix((n_docs,n_docs))\n",
    "\n",
    "docs_idx=[]\n",
    "docs_val=[]\n",
    "for j in trange(n_docs):\n",
    "    docs_idx.append(inv_idx.idx_mtx[:,j].nonzero()[0])\n",
    "    docs_val.append(np.asarray(inv_idx.idx_mtx[docs_idx[j],j].todense()).reshape(-1))\n",
    "# len(docs_idx), docs_idx[:2]\n",
    "# len(docs_val), docs_val[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similitud Coseno. Procesamiento y visualización.\n",
    "Se calcula la similitud coseno entre los 1000 queries y los documentos en los que se encuentran sus palabras.\n",
    "\n",
    "Se saca provecho del hecho de que los vectores de los documentos generados por `TfidfVectorizer` al usar la normalización `norm='l2'` están ya normalizados (su longitud es 1), por esta razón se puede ignorar el divisor de la fórmula de la simitud coseno:\n",
    "\n",
    "\\begin{equation}\n",
    "\\cos ({u},{v})= {{u}*{v} \\over \\|{u}\\| \\|{u}\\|} = \\frac{ \\sum_{i=1}^{n}{{u}_i{v}_i} }{ \\sqrt{\\sum_{i=1}^{n}{({u}_i)^2}} \\sqrt{\\sum_{i=1}^{n}{({v}_i)^2}} }\n",
    "\\end{equation}\n",
    "\n",
    "Quedando de la siguiente manera:\n",
    "\n",
    "\\begin{equation}\n",
    "\\cos ({u},{v})= {{u}*{v} \\over \\|{u}\\| \\|{u}\\|} = \\sum_{i=1}^{n}{{u}_i{v}_i} = u \\cdot v\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_queries(qs):\n",
    "    \"\"\"\n",
    "    Se hace una consulta OR con el query query.    \n",
    "    Se obtiene la representación BOW de índices y valores del query.    \n",
    "    Se obtiene la representación BOW de índices y valores de los documentos resultado de la consulta OR.\n",
    "    Se calcula la similitud coseno entre el query y los documentos.\n",
    "    Se acumulan el valor de la similitud coseno recién calculada. \n",
    "    Se grafican los valores acumulados en el histograma.\n",
    "    \"\"\"\n",
    "    sims=[]\n",
    "    tx = datetime.datetime.now()\n",
    "    tqdm.write(f\"Preprocessed {count_nonzero_triangular(docs_prods)} of {n_pairs} docs pairs.\")\n",
    "    n=0\n",
    "    for i in tqdm(qs):\n",
    "        q=inv_idx.tokenized_docs[i]\n",
    "        ds=inv_idx.o(q) # Consulta OR.\n",
    "        qi=docs_idx[i]\n",
    "        qv=docs_val[i]\n",
    "        qi_set=set(qi)\n",
    "        for j in ds:\n",
    "            if i==j: continue\n",
    "            dot_sum=0\n",
    "            # Si existe el valor calculado se consulta y no se calcula de nuevo.\n",
    "            if docs_prods[i,j] != 0: \n",
    "                dot_sum=docs_prods[i,j]\n",
    "            # Si no existe se calcula.\n",
    "            else:\n",
    "                # Se recuperan los vectores preprocesados.\n",
    "                di=docs_idx[j]\n",
    "                dv=docs_val[j]\n",
    "                # Multiplicación de cada elemento.\n",
    "                a=[qv[np.where(qi==x)]*dv[np.where(di==x)] for x in qi_set.intersection(di)]\n",
    "                # Suma de todos los elelemtos.\n",
    "                dot_sum=np.sum(a)\n",
    "                # Se almancena el valor en una matriz de manera simétrica.\n",
    "                docs_prods[i,j]=docs_prods[j,i]=dot_sum\n",
    "            sims.append(dot_sum) # Se almacena el producto punto para no calcularlo de nuevo.\n",
    "            \n",
    "        tx,label=show_progress(100, tx, n, ret_string=True)\n",
    "        if label is not None: \n",
    "            tqdm.write(label + f\"\\n\\tQuery {n} of {len(qs)} with {len(ds)} results. \" +\n",
    "                   f\"Preprocessed {count_nonzero_triangular(docs_prods)} of {n_pairs} docs pairs.\")\n",
    "        n+=1\n",
    "    \n",
    "    _=plt.hist(sims, bins=list(np.arange(0,0.4,0.01)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESULTADOS \n",
    "### Poda, tiempo de consulta e histogramas\n",
    "\n",
    "Se eligen 1000 documentos de los 100,000 disponibles, para posteriormente realizar las búsquedas.\n",
    "\n",
    "Se procede a generar y luego podar las listas de posteo para las longuitudes 200, 150, 100, 50 y 0 (sin poda).\n",
    "\n",
    "También se mide el tiempo de consulta de los 1000 documentos y se grafica el histograma de los datos agregados en cada caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "k = 1000 # Documentos a elegir.\n",
    "qs = random.sample(range(0,len(inv_idx.tokenized_docs)), k)\n",
    "queries = [inv_idx.tokenized_docs[i] for i in qs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sin poda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_=inv_idx.compute_plists()\n",
    "print(f\"Longitud de posting list 'amlo'= {len(inv_idx.postlists[process_line('amlo')[0]])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for q in queries:\n",
    "    _=inv_idx.o(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_queries(qs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Poda 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_=inv_idx.compute_plists(200)\n",
    "print(f\"Longitud de posting list 'amlo'= {len(inv_idx.postlists[process_line('amlo')[0]])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for q in queries:\n",
    "    _=inv_idx.o(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_queries(qs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Poda 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_=inv_idx.compute_plists(150)\n",
    "print(f\"Longitud de posting list 'amlo'= {len(inv_idx.postlists[process_line('amlo')[0]])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for q in queries:\n",
    "    _=inv_idx.o(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_queries(qs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Poda 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_=inv_idx.compute_plists(100)\n",
    "print(f\"Longitud de posting list 'amlo'= {len(inv_idx.postlists[process_line('amlo')[0]])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for q in queries:\n",
    "    _=inv_idx.o(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_queries(qs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Poda 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_=inv_idx.compute_plists(50)\n",
    "print(f\"Longitud de posting list 'amlo'= {len(inv_idx.postlists[process_line('amlo')[0]])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for q in queries:\n",
    "    _=inv_idx.o(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_queries(qs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se listan todos los archivos .pickle. Se puede notar el tamaño para cada uno que corresponde para una longitud de poda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh *.pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusiones\n",
    "* Uno de los principales retos al desarrollar esta actividad fue encontrar soluciones que mejoraran el desempeño. Sobre todo del cálculo de la similitud coseno de tantos documentos y considerando tantas consultas. Se calculó que las implementaciones iniciales tadrarían cerca de 20 horas para procesar los datos necesarios para graficar los histogramas solicitados. Se diagnosticó con el uso del `line_profiler` y `%lprun` los statements que consumían más tiempo de procesamiento, lo cual llevó a evitar el uso masivo de la conversión de una matriz o vector disperso en uno denso (método `todense()`). Derivado de lo anterior las principales optimizaciones hechas son las siguientes:\n",
    "    * Representar el vector BOW de manera simplificada, en vez de ser un vector disperso con muchos ceros.\n",
    "    * Preprocesar dichas representaciones simplificadas. Para procesarlas una sola vez.\n",
    "    * Se notó que el acceso a una lista de numpy arrays es mucho más eficiente que a una lista de listas. Se uso esa estructura para almacenar los objetos críticos.\n",
    "    * Procesar el producto punto entre dos documentos solamente una vez. Para ello se calcula la primera vez y se almacena para que la segunda vez que se requiera solo se recupera desde memoria.\n",
    "* Es de notarse que las consultas son más rápidas cuando se realiza poda, como era de esperarse. Conforme la longitud de poda es menor también es menor el tiempo requerido para realizar las consultas.<br>Las diferentes longitudes de poda no muestran un impacto significativo en esta medición. Esto puede deberse a la amplia diferencia en la longitud con las listas de posteo originales (por ejemplo 19,321 elementos en lista de _\"amlo\"_ ).\n",
    "* El tamaño de las longitud de poda de las listas de posteo es directamente proporcional a la cantidad de memoria requerida. Menor longitud, menor tamaño. \n",
    "* Las similitudes entre los documentos de consulta y sus resultados tienden a alejarse de 0 conforme la longitud de poda decrece, según se puede apreciar en los histogramas para cada caso. Además en todos los casos se mantiene una distribución con incidencias concentradas cerca de un valor central y que decrecen conforme se alejan de ese valor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referencias\n",
    "\n",
    "\t[1]\t“Inverted Index - ML Wiki,” Mlwiki.org, 2017. [Online]. Available: http://mlwiki.org/index.php/Inverted_Index. [Accessed: 18-Apr-2020].<br>\n",
    "\t[2]\t“sklearn.feature_extraction.text.TfidfVectorizer — scikit-learn 0.22.2 documentation,” Scikit-learn.org, 2019. [Online]. Available: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html. [Accessed: 18-Apr-2020].<br>\n",
    "\t[3]\t“Sparse matrices (scipy.sparse) — SciPy v1.4.1 Reference Guide,” Scipy.org, 2019. [Online]. Available: https://docs.scipy.org/doc/scipy/reference/sparse.html. [Accessed: 18-Apr-2020].<br>\n",
    "\t[4]\tMausam, “Document Similarity in Information Retrieval.” [Online]. Available: https://courses.cs.washington.edu/courses/cse573/12sp/lectures/17-ir.pdf.<br>\n",
    "\t[5]\t“sklearn.feature_extraction.text.TfidfTransformer — scikit-learn 0.22.2 documentation,” Scikit-learn.org, 2011. [Online]. Available: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer. [Accessed: 18-Apr-2020]."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
