{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple outputs per cell in Jupyter \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "// Evitar autoscroll.\n",
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "// Evitar autoscroll.\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 4), (3, 5), (2, 6)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[(1, 3, 2), (4, 5, 6)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[1, 3, 2]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[4, 5, 6]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a = [1,2,3]\n",
    "# b = [4,5,6]\n",
    "# zipped_list = zip(a,b)\n",
    "zipped_list=[(1, 4), (3, 5), (2, 6)]\n",
    "zipped_list\n",
    "unzipped_list=list(zip(*zipped_list))\n",
    "unzipped_list\n",
    "list(unzipped_list[0])\n",
    "list(unzipped_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 4), (2, 6), (3, 5)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[(1, 2, 3), (4, 6, 5)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3], [4, 6, 5]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=[(1, 4), (3, 5), (2, 6)]\n",
    "\n",
    "p.sort()\n",
    "p\n",
    "\n",
    "p=list(zip(*p))\n",
    "p\n",
    "\n",
    "p=[list(p[0]),list(p[1])]\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# from toolbox import *\n",
    "import toolbox as tb\n",
    "from skeleton import *\n",
    "import pandas as pd\n",
    "from gensim.models import FastText\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `tokenize` not found.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokens():\n",
    "    tokenized_pointer=None\n",
    "    \n",
    "    def tokenize(self,filename,tokenized_filename='tokenized.npy',replace=False):\n",
    "        self.tokenized_filename=tokenized_filename\n",
    "        # Si existe el archivo.\n",
    "        if os.path.isfile(self.tokenized_filename) and not replace:\n",
    "            self.tokenized_pointer= np.load(self.tokenized_filename, mmap_mode='r')   \n",
    "            print(f\"** Processed. {os.path.abspath(self.tokenized_filename)} already exists, then loaded.\") \n",
    "            return self.tokenized_pointer\n",
    "\n",
    "        \n",
    "        if not replace and tokenized_pointer:\n",
    "            print(f\"** tokenized_pointer is already loaded.\")\n",
    "            return self.tokenized_pointer\n",
    "        \n",
    "        \n",
    "        \n",
    "        if replace:\n",
    "            print(\"** Replacing...\")\n",
    "        elif tokenized_pointer:\n",
    "            print(f\"** tokenized_pointer is already loaded.\")\n",
    "            return self.tokenized_pointer\n",
    "        \n",
    "        if os.path.isfile(self.tokenized_filename):\n",
    "            print(f\"** Replace is off. {os.path.abspath(self.tokenized_filename)} already exists, then load.\")\n",
    "        else:\n",
    "            print(f\"** Replace is off. {os.path.abspath(self.tokenized_filename)} already exists, then load.\")\n",
    "        \n",
    "        if replace or not tokenized_pointer or os.path.isfile(self.tokenized_filename):\n",
    "            tokenized_docs=[]\n",
    "            self.N=0\n",
    "            print(f\"** Processing {filename} ...\")\n",
    "            for idx, tw in tqdm(enumerate(tweet_iterator(filename))):\n",
    "                twTokens = tb.process_line(tw['text']) # Limpiando text.\n",
    "                tokenized_docs.append(twTokens)\n",
    "            self.N=idx+1\n",
    "\n",
    "            np.save(tokenized_filename,tokenized_docs)\n",
    "            del(tokenized_docs)\n",
    "        \n",
    "        self.tokenized_pointer=np.load(self.tokenized_filename, mmap_mode='r')\n",
    "        print(f\"** Processed {self.N} lines. Saved to {os.path.abspath(self.tokenized_filename)}.\")\n",
    "        return self.tokenized_pointer\n",
    "    \n",
    "    def slice(s)\n",
    "        return self.tokenized_filename[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Index():\n",
    "#     vocab = None\n",
    "    N = 0\n",
    "    DF = Counter({})\n",
    "    TF = []\n",
    "    \n",
    "#     DF_vec = None\n",
    "#     IDF_vec = None\n",
    "\n",
    "    def computePostingLists(self, filename, showProgressEach=None):\n",
    "        print(\"** Processing \" + filename + \"...\")\n",
    "#         tx = datetime.datetime.now()\n",
    "        for idx, tw in tqdm(enumerate(tweet_iterator(filename))):\n",
    "#             tb.show_progress(showProgressEach,tx,idx)\n",
    "            twTxtLst = tb.process_line(tw['text']) # Limpiando text.\n",
    "            \n",
    "            # Sumando frecuencias individiales.\n",
    "            twCnt = Counter(twTxtLst)\n",
    "            \n",
    "            TF.append(twCnt)\n",
    "            \n",
    "            # Sumando frecuencias por documento (DF).\n",
    "            self.DF.update(list(twCnt.keys()))\n",
    "\n",
    "            # Contabilizando documentos.\n",
    "            self.N += 1\n",
    "            \n",
    "#         words = self.DF.keys()\n",
    "#         self.vocab = dict(zip(words, range(len(words))))\n",
    "#         self.DF_vec = cnt2vec(self.DF, self.vocab)\n",
    "#         self.IDF_vec = np.log2(self.N / (self.DF_vec + 1))\n",
    "        \n",
    "#         # Calculando TF-IDF y asignando a usuarios.\n",
    "#         for userId in self.users:\n",
    "#             TF_cnt = self.users[userId]\n",
    "#             self.users[userId] = cnt2vec(TF_cnt, self.vocab) * self.IDF_vec\n",
    "        postlists=defaultdict(list)\n",
    "\n",
    "    def getX(self):\n",
    "        return np.array([self.users[key] for key in self.users])\n",
    "    \n",
    "    def cluster(self, n_clusters=9):\n",
    "        self.cluster = KMeans(n_clusters=n_clusters, n_jobs=4).fit(self.getX())\n",
    "        return self.cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj=np.arange(100*50000).reshape((50000,100))\n",
    "np.save(\"obj.npy\",obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 100)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.load(\"obj.npy\", mmap_mode='r')\n",
    "X.shape\n",
    "# X = np.load(\"obj.npy\")\n",
    "# for i in range(20):\n",
    "#     print(X[i, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__name__ 57\n",
      "__doc__ 113\n",
      "__package__ 16\n",
      "__loader__ 16\n",
      "__spec__ 16\n",
      "__builtin__ 88\n",
      "__builtins__ 88\n",
      "_ih 200\n",
      "_oh 248\n",
      "_dh 80\n",
      "In 200\n",
      "Out 248\n",
      "get_ipython 72\n",
      "exit 64\n",
      "quit 64\n",
      "_ 72\n",
      "__ 49\n",
      "___ 49\n",
      "_i 162\n",
      "_ii 154\n",
      "_iii 154\n",
      "_i1 195\n",
      "InteractiveShell 1064\n",
      "_i2 167\n",
      "_i3 225\n",
      "np 88\n",
      "tb 88\n",
      "skmeans 1064\n",
      "transforms 88\n",
      "plt 88\n",
      "cm 88\n",
      "nltk 88\n",
      "itertools 88\n",
      "string 88\n",
      "unicodedata 88\n",
      "re 88\n",
      "word_tokenize 144\n",
      "PCA 1064\n",
      "pd 88\n",
      "faiss 88\n",
      "axes3d 88\n",
      "punctuation_table 1192\n",
      "stop_words 2904\n",
      "euclidiana 144\n",
      "coseno 144\n",
      "plotClusters 144\n",
      "plotPCA 144\n",
      "preprocess 144\n",
      "tokenize_sentences 144\n",
      "cocurrency_matrix 144\n",
      "plotDecisionBoundary 144\n",
      "NearestCentroid 1064\n",
      "kNN 1064\n",
      "Clustering 1064\n",
      "FastText 1064\n",
      "_i4 58\n",
      "_i5 154\n",
      "X 144\n",
      "_i6 117\n",
      "obj 112\n",
      "_i7 154\n",
      "_i8 154\n",
      "_i9 162\n",
      "_9 72\n",
      "_i10 210\n",
      "sys 88\n",
      "Total = 18206\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "local_vars = list(locals().items())\n",
    "tot=0\n",
    "for var, obj in local_vars:\n",
    "    mem=sys.getsizeof(obj)\n",
    "    tot+=mem\n",
    "    print(var, mem)\n",
    "print(\"Total =\",tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
