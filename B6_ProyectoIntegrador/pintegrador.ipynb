{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple outputs per cell in Jupyter \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "// Evitar autoscroll.\n",
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "// Evitar autoscroll.\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 4), (3, 5), (2, 6)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[(1, 3, 2), (4, 5, 6)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[1, 3, 2]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[4, 5, 6]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a = [1,2,3]\n",
    "# b = [4,5,6]\n",
    "# zipped_list = zip(a,b)\n",
    "zipped_list=[(1, 4), (3, 5), (2, 6)]\n",
    "zipped_list\n",
    "unzipped_list=list(zip(*zipped_list))\n",
    "unzipped_list\n",
    "list(unzipped_list[0])\n",
    "list(unzipped_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 4), (2, 6), (3, 5)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[(1, 2, 3), (4, 6, 5)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3], [4, 6, 5]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=[(1, 4), (3, 5), (2, 6)]\n",
    "\n",
    "p.sort()\n",
    "p\n",
    "\n",
    "p=list(zip(*p))\n",
    "p\n",
    "\n",
    "p=[list(p[0]),list(p[1])]\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# from toolbox import *\n",
    "import toolbox as tb\n",
    "from skeleton import *\n",
    "import pandas as pd\n",
    "from gensim.models import FastText\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import tqdm\n",
    "from microtc.utils import tweet_iterator\n",
    "import datetime\n",
    "from collections import Counter\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokens():\n",
    "    tokenized_pointer=None\n",
    "    \n",
    "    def tokenize(self,json_file,npy_file=None,replace=False):\n",
    "        self.npy_file=str(os.path.splitext(json_file)[0]+\".npy\") if not npy_file else npy_file\n",
    "\n",
    "        if not replace and os.path.isfile(self.npy_file):\n",
    "            print(f\"** Replace is off. {os.path.abspath(self.npy_file)} already exists, then load.\")\n",
    "        else:\n",
    "            tokenized_docs=[]\n",
    "            self.N=0\n",
    "            print(f\"** Processing {json_file} ...\")\n",
    "            tx = datetime.datetime.now()\n",
    "            for idx, tw in enumerate(tweet_iterator(json_file)):\n",
    "                tb.show_progress(1000, tx, idx)\n",
    "                twTokens = tb.process_line(tw['text']) # Limpiando text.\n",
    "#                 print(type(twTokens))\n",
    "                tokenized_docs.append(twTokens)\n",
    "            \n",
    "#             print(\"max:\",len(max(tokenized_docs)))\n",
    "    \n",
    "            self.N=idx+1\n",
    "#             print(type(tokenized_docs),len(tokenized_docs))\n",
    "#             print(tokenized_docs)\n",
    "#             tokenized_docs\n",
    "            maxLen=len(max(tokenized_docs))\n",
    "            for i, doc in enumerate(tokenized_docs):\n",
    "#                 print(\"Before:\",tokenized_docs[i])\n",
    "#                 print(doc, ['']*(maxLen - len(doc)))\n",
    "#                 tokenized_docs[i] = doc + ['']*(maxLen - len(doc))\n",
    "                tokenized_docs[i]=[doc[x] if x<len(doc) else '' for x in range(maxLen)]\n",
    "#                 print(\"After:\",tokenized_docs[i])\n",
    "            \n",
    "            np.save(self.npy_file,tokenized_docs)\n",
    "            del(tokenized_docs)\n",
    "            gc.collect()\n",
    "            print(f\"** Processed {self.N} lines. Saved to {os.path.abspath(self.npy_file)}.\")\n",
    "        \n",
    "        self.pointer=np.load(self.npy_file, mmap_mode='r')\n",
    "        return self\n",
    "    \n",
    "    def get(self,n):\n",
    "        return [x for x in self.pointer[n,:] if x != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Processing data/geo-mx-2004_min.json ...\n",
      "2020-05-12 03:23:16.540113 :: 0.000166 - Processing item #0 \n",
      "** Processed 10 lines. Saved to /home/jovyan/public/B6_ProyectoIntegrador/data/geo-mx-2004_min.npy.\n"
     ]
    }
   ],
   "source": [
    "tokens=Tokens().tokenize(\"data/geo-mx-2004_min.json\",replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nochecit']"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.get(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Index():\n",
    "#     vocab = None\n",
    "    N = 0\n",
    "    DF = Counter({})\n",
    "    TF = []\n",
    "    \n",
    "#     DF_vec = None\n",
    "#     IDF_vec = None\n",
    "\n",
    "    def computePostingLists(self, filename, showProgressEach=None):\n",
    "        print(\"** Processing \" + filename + \"...\")\n",
    "#         tx = datetime.datetime.now()\n",
    "        for idx, tw in tqdm(enumerate(tweet_iterator(filename))):\n",
    "#             tb.show_progress(showProgressEach,tx,idx)\n",
    "            twTxtLst = tb.process_line(tw['text']) # Limpiando text.\n",
    "            \n",
    "            # Sumando frecuencias individiales.\n",
    "            twCnt = Counter(twTxtLst)\n",
    "            \n",
    "            TF.append(twCnt)\n",
    "            \n",
    "            # Sumando frecuencias por documento (DF).\n",
    "            self.DF.update(list(twCnt.keys()))\n",
    "\n",
    "            # Contabilizando documentos.\n",
    "            self.N += 1\n",
    "            \n",
    "#         words = self.DF.keys()\n",
    "#         self.vocab = dict(zip(words, range(len(words))))\n",
    "#         self.DF_vec = cnt2vec(self.DF, self.vocab)\n",
    "#         self.IDF_vec = np.log2(self.N / (self.DF_vec + 1))\n",
    "        \n",
    "#         # Calculando TF-IDF y asignando a usuarios.\n",
    "#         for userId in self.users:\n",
    "#             TF_cnt = self.users[userId]\n",
    "#             self.users[userId] = cnt2vec(TF_cnt, self.vocab) * self.IDF_vec\n",
    "        postlists=defaultdict(list)\n",
    "\n",
    "    def getX(self):\n",
    "        return np.array([self.users[key] for key in self.users])\n",
    "    \n",
    "    def cluster(self, n_clusters=9):\n",
    "        self.cluster = KMeans(n_clusters=n_clusters, n_jobs=4).fit(self.getX())\n",
    "        return self.cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total = 128084\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "local_vars = list(locals().items())\n",
    "tot=0\n",
    "vars_=[]\n",
    "for var, obj in local_vars:\n",
    "    mem=sys.getsizeof(obj)\n",
    "    tot+=mem\n",
    "    vars_.append((mem,var))\n",
    "print(\"Total =\",tot)\n",
    "print(vars_.sort())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
