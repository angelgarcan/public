{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple outputs per cell in Jupyter \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "// Evitar autoscroll.\n",
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "// Evitar autoscroll.\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 4), (3, 5), (2, 6)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[(1, 3, 2), (4, 5, 6)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[1, 3, 2]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[4, 5, 6]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a = [1,2,3]\n",
    "# b = [4,5,6]\n",
    "# zipped_list = zip(a,b)\n",
    "zipped_list=[(1, 4), (3, 5), (2, 6)]\n",
    "zipped_list\n",
    "unzipped_list=list(zip(*zipped_list))\n",
    "unzipped_list\n",
    "list(unzipped_list[0])\n",
    "list(unzipped_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 4), (2, 6), (3, 5)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[(1, 2, 3), (4, 6, 5)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3], [4, 6, 5]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=[(1, 4), (3, 5), (2, 6)]\n",
    "\n",
    "p.sort()\n",
    "p\n",
    "\n",
    "p=list(zip(*p))\n",
    "p\n",
    "\n",
    "p=[list(p[0]),list(p[1])]\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# from toolbox import *\n",
    "import toolbox as tb\n",
    "from skeleton import *\n",
    "import pandas as pd\n",
    "from gensim.models import FastText\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import tqdm\n",
    "from microtc.utils import tweet_iterator\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokens():\n",
    "    tokenized_pointer=None\n",
    "    \n",
    "    def tokenize(self,json_file,npy_file=None,replace=False):\n",
    "        print(\"NOT\" if not npy_file else \"YES\")\n",
    "        print(os.path.splitext(json_file)[0]+'.npy')\n",
    "        print(str(os.path.splitext(json_file)[0]+\".npy\") if not npy_file else npy_file)\n",
    "        self.npy_file=str(os.path.splitext(json_file)[0]+\".npy\") if not npy_file else npy_file\n",
    "        print(\"NOT\" if not self.npy_file else \"YES\")\n",
    "        print(self.npy_file)\n",
    "\n",
    "        if not replace and os.path.isfile(self.npy_file):\n",
    "            print(f\"** Replace is off. {os.path.abspath(self.npy_file)} already exists, then load.\")\n",
    "        else:\n",
    "            tokenized_docs=[]\n",
    "            self.N=0\n",
    "            print(f\"** Processing {json_file} ...\")\n",
    "            tx = datetime.datetime.now()\n",
    "            for idx, tw in enumerate(tweet_iterator(json_file)):\n",
    "                tb.show_progress(1000, tx, idx)\n",
    "                twTokens = tb.process_line(tw['text']) # Limpiando text.\n",
    "                tokenized_docs.append(twTokens)\n",
    "            self.N=idx+1\n",
    "\n",
    "            np.save(self.npy_file,tokenized_docs)\n",
    "            del(tokenized_docs)\n",
    "            print(f\"** Processed {self.N} lines. Saved to {os.path.abspath(self.npy_file)}.\")\n",
    "        \n",
    "        self.tokenized_pointer=np.load(self.npy_file, mmap_mode='r')\n",
    "        return self\n",
    "    \n",
    "    def get(s):\n",
    "        return self.tokenized_pointer[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT\n",
      "data/geo-mx-2004_min.npy\n",
      "data/geo-mx-2004_min.npy\n",
      "YES\n",
      "data/geo-mx-2004_min.npy\n",
      "** Processing data/geo-mx-2004_min.json ...\n",
      "2020-05-12 02:09:26.994961 :: 0.000709 - Processing item #0 \n",
      "** Processed 1000 lines. Saved to /home/jovyan/public/B6_ProyectoIntegrador/data/geo-mx-2004_min.npy.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Array can't be memory-mapped: Python objects in dtype.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-c1434908189c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/geo-mx-2004_min.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-51-21dea37ed9c6>\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, json_file, npy_file, replace)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"** Processed {self.N} lines. Saved to {os.path.abspath(self.npy_file)}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenized_pointer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnpy_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0;31m# .npy file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_memmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmmap_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    451\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m                 return format.read_array(fid, allow_pickle=allow_pickle,\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mopen_memmap\u001b[0;34m(filename, mode, dtype, shape, fortran_order, version)\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Array can't be memory-mapped: Python objects in dtype.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m             \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Array can't be memory-mapped: Python objects in dtype."
     ]
    }
   ],
   "source": [
    "tokens=Tokens().tokenize(\"data/geo-mx-2004_min.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Index():\n",
    "#     vocab = None\n",
    "    N = 0\n",
    "    DF = Counter({})\n",
    "    TF = []\n",
    "    \n",
    "#     DF_vec = None\n",
    "#     IDF_vec = None\n",
    "\n",
    "    def computePostingLists(self, filename, showProgressEach=None):\n",
    "        print(\"** Processing \" + filename + \"...\")\n",
    "#         tx = datetime.datetime.now()\n",
    "        for idx, tw in tqdm(enumerate(tweet_iterator(filename))):\n",
    "#             tb.show_progress(showProgressEach,tx,idx)\n",
    "            twTxtLst = tb.process_line(tw['text']) # Limpiando text.\n",
    "            \n",
    "            # Sumando frecuencias individiales.\n",
    "            twCnt = Counter(twTxtLst)\n",
    "            \n",
    "            TF.append(twCnt)\n",
    "            \n",
    "            # Sumando frecuencias por documento (DF).\n",
    "            self.DF.update(list(twCnt.keys()))\n",
    "\n",
    "            # Contabilizando documentos.\n",
    "            self.N += 1\n",
    "            \n",
    "#         words = self.DF.keys()\n",
    "#         self.vocab = dict(zip(words, range(len(words))))\n",
    "#         self.DF_vec = cnt2vec(self.DF, self.vocab)\n",
    "#         self.IDF_vec = np.log2(self.N / (self.DF_vec + 1))\n",
    "        \n",
    "#         # Calculando TF-IDF y asignando a usuarios.\n",
    "#         for userId in self.users:\n",
    "#             TF_cnt = self.users[userId]\n",
    "#             self.users[userId] = cnt2vec(TF_cnt, self.vocab) * self.IDF_vec\n",
    "        postlists=defaultdict(list)\n",
    "\n",
    "    def getX(self):\n",
    "        return np.array([self.users[key] for key in self.users])\n",
    "    \n",
    "    def cluster(self, n_clusters=9):\n",
    "        self.cluster = KMeans(n_clusters=n_clusters, n_jobs=4).fit(self.getX())\n",
    "        return self.cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj=np.arange(100*50000).reshape((50000,100))\n",
    "np.save(\"obj.npy\",obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 100)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.load(\"obj.npy\", mmap_mode='r')\n",
    "X.shape\n",
    "# X = np.load(\"obj.npy\")\n",
    "# for i in range(20):\n",
    "#     print(X[i, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__name__ 57\n",
      "__doc__ 113\n",
      "__package__ 16\n",
      "__loader__ 16\n",
      "__spec__ 16\n",
      "__builtin__ 88\n",
      "__builtins__ 88\n",
      "_ih 200\n",
      "_oh 248\n",
      "_dh 80\n",
      "In 200\n",
      "Out 248\n",
      "get_ipython 72\n",
      "exit 64\n",
      "quit 64\n",
      "_ 72\n",
      "__ 49\n",
      "___ 49\n",
      "_i 162\n",
      "_ii 154\n",
      "_iii 154\n",
      "_i1 195\n",
      "InteractiveShell 1064\n",
      "_i2 167\n",
      "_i3 225\n",
      "np 88\n",
      "tb 88\n",
      "skmeans 1064\n",
      "transforms 88\n",
      "plt 88\n",
      "cm 88\n",
      "nltk 88\n",
      "itertools 88\n",
      "string 88\n",
      "unicodedata 88\n",
      "re 88\n",
      "word_tokenize 144\n",
      "PCA 1064\n",
      "pd 88\n",
      "faiss 88\n",
      "axes3d 88\n",
      "punctuation_table 1192\n",
      "stop_words 2904\n",
      "euclidiana 144\n",
      "coseno 144\n",
      "plotClusters 144\n",
      "plotPCA 144\n",
      "preprocess 144\n",
      "tokenize_sentences 144\n",
      "cocurrency_matrix 144\n",
      "plotDecisionBoundary 144\n",
      "NearestCentroid 1064\n",
      "kNN 1064\n",
      "Clustering 1064\n",
      "FastText 1064\n",
      "_i4 58\n",
      "_i5 154\n",
      "X 144\n",
      "_i6 117\n",
      "obj 112\n",
      "_i7 154\n",
      "_i8 154\n",
      "_i9 162\n",
      "_9 72\n",
      "_i10 210\n",
      "sys 88\n",
      "Total = 18206\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "local_vars = list(locals().items())\n",
    "tot=0\n",
    "for var, obj in local_vars:\n",
    "    mem=sys.getsizeof(obj)\n",
    "    tot+=mem\n",
    "    print(var, mem)\n",
    "print(\"Total =\",tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
