{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple outputs per cell in Jupyter \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%javascript\n",
    "# // Evitar autoscroll.\n",
    "# IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "#     return false;\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from skeleton import *\n",
    "from sklearn.metrics import (recall_score, f1_score,\n",
    "                             precision_score, accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-bbd817028efc>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-bbd817028efc>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    {:.2f}\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def testClassifier(clf,train,y_train,test,y_test):\n",
    "    print(\"=== \",clf.__class__.__name__,end='')\n",
    "    print(\":\",vars(clf))\n",
    "    clf=clf.fit(train,y_train)\n",
    "    #Predecimos las etiqueta del conjunto de prueba\n",
    "    yp=clf.predict(test)\n",
    "    # Medidas de bondad\n",
    "    scores={}\n",
    "\n",
    "    scores['Accuracy']=accuracy_score(y_test,yp)\n",
    "    scores['Macro F1']=f1_score(y_test,yp,average='macro')\n",
    "    scores['Macro Recall']=recall_score(y_test,yp,average='macro')\n",
    "    scores['F1 is_humor=1']=f1_score(y_test,yp)\n",
    "    \n",
    "    print(\"Accuracy\",\"Macro F1\",\"Macro Recall\",\"F1 is_humor=1\",sep='\\t')\n",
    "    print(\"{:.5f}\\t{:.5f}\\t{:.5f}\\t{:.5f}\".format(\n",
    "        scores['Accuracy'],scores['Macro F1'],scores['Macro Recall'],scores['F1 is_humor=1']))\n",
    "                            \n",
    "#     print(\"Recall: \", scores['recall'])\n",
    "#     print(\"F1: \", scores['f1'])\n",
    "#     print(\"Accuracy: \", scores['accuracy'])\n",
    "#     print(\"F1 Humor: \", scores['f1_humor'])\n",
    "    #Vemos la frontera de decisión\n",
    "    plotDecisionBoundary(clf,train, y_train)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un problema de texto  (Identificación de humor HAHA -2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este ejemplo utilizaremos los datos proporcionados para la tarea 1 del HAHA - Humor Analysis based on Human Annotation, la cual consiste determinar si tweets en español son humorísticos o no (un problema de clasificación binaria). Esta tarea forma parte del Iberian Languages Evaluation Forum (IberLEF 2019)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus \n",
    "\n",
    "El conjunto de datos fue etiquetado como se indica [5]. Todos los tweets están etiquetados como humorístico **is_humor=1**  o no humorístico **is_humor=0**. Los datos relevantes para el problema se clasificación ser muestran en el siguiente ejemplo:\n",
    "\n",
    "|Desscripción|Clave|Valor|\n",
    "|:------:|:------:|:-----:|\n",
    "|Tweet |text|Después de la tormenta sale... Tu mamá gritando porque no metiste la ropa.|\n",
    "|Etiqueta de clase|is_humor|1|\n",
    "|Representación vectorial|vec| Vectores de dimensión 300 (FastText preentrenados para español) |\n",
    "|Identificador|id|942079817905770496|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos lod datos.\n",
    "train_data=pd.read_json('data/haha_train_ft_pre_min10.json',lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vemos los primeros 5\n",
    "train_data.head()[['id','text','is_humor','vec']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dimensión de los vectores\n",
    "len(train_data.vec[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los datos de prueba\n",
    "test_data=pd.read_json('data/haha_test_ft_pre_min10.json',lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtenemos la representación vectorial para los conjunto de prueba y entrenamiento\n",
    "train,y_train=np.array([np.array(x) for x in train_data.vec]),np.array(train_data.is_humor)\n",
    "test,y_test=np.array([np.array(x) for x in test_data.vec]),np.array(test_data.is_humor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P={\"FastText\":{\"Accuracy\":0.80905,\"Macro F1\":0.79327,\"Macro Recall\":0.78690,\"F1 is_humor=1\":0.73614}} #Performance\n",
    "\n",
    "P[\"NC.cos.avg\"]=testClassifier(NearestCentroid(distance='coseno',centroid_type='Average'),train,y_train,test,y_test)\n",
    "P[\"NC.cos.sum\"]=testClassifier(NearestCentroid(distance='coseno',centroid_type='Sum'),train,y_train,test,y_test)\n",
    "P[\"NC.cos.rocchio\"]=testClassifier(NearestCentroid(distance='coseno',centroid_type='Rocchio'),train,y_train,test,y_test)\n",
    "P[\"NC.cos.nsum\"]=testClassifier(NearestCentroid(distance='coseno',centroid_type='NormSum'),train,y_train,test,y_test)\n",
    "\n",
    "P[\"KNN.cos.u\"]=testClassifier(kNN(distance='coseno',k=5,weight_type='uniform'),train,y_train,test,y_test)\n",
    "P[\"KNN.cos.m\"]=testClassifier(kNN(distance='coseno',k=5,weight_type='mean_dist'),train,y_train,test,y_test)\n",
    "P[\"KNN.cos.w\"]=testClassifier(kNN(distance='coseno',k=5,weight_type='weighed_dist'),train,y_train,test,y_test)\n",
    "\n",
    "P[\"NC.euc.avg\"]=testClassifier(NearestCentroid(distance='euclidiana',centroid_type='Average'),train,y_train,test,y_test)\n",
    "P[\"NC.euc.sum\"]=testClassifier(NearestCentroid(distance='euclidiana',centroid_type='Sum'),train,y_train,test,y_test)\n",
    "P[\"NC.euc.rocchio\"]=testClassifier(NearestCentroid(distance='euclidiana',centroid_type='Rocchio'),train,y_train,test,y_test)\n",
    "P[\"NC.euc.nsum\"]=testClassifier(NearestCentroid(distance='euclidiana',centroid_type='NormSum'),train,y_train,test,y_test)\n",
    "\n",
    "P[\"KNN.euc.u\"]=testClassifier(kNN(distance='euclidiana',k=5,weight_type='uniform'),train,y_train,test,y_test)\n",
    "P[\"KNN.euc.m\"]=testClassifier(kNN(distance='euclidiana',k=5,weight_type='mean_dist'),train,y_train,test,y_test)\n",
    "P[\"KNN.euc.w\"]=testClassifier(kNN(distance='euclidiana',k=5,weight_type='weighed_dist'),train,y_train,test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(P).T.sort_values(\"f1_humor\",ascending =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
