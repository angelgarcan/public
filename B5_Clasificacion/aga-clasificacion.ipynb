{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple outputs per cell in Jupyter \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "// Evitar autoscroll.\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from skeleton import *\n",
    "from sklearn.metrics import (recall_score, f1_score,\n",
    "                             precision_score, accuracy_score)\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testClassifier(clf,train,y_train,test,y_test):\n",
    "    print(\"=== \",clf.__class__.__name__,end='')\n",
    "    print(\":\",vars(clf))\n",
    "    clf=clf.fit(train,y_train)\n",
    "    #Predecimos las etiqueta del conjunto de prueba\n",
    "    yp=clf.predict(test)\n",
    "    # Medidas de bondad\n",
    "    scores={}\n",
    "\n",
    "    scores['Accuracy']=accuracy_score(y_test,yp)\n",
    "    scores['Macro F1']=f1_score(y_test,yp,average='macro')\n",
    "    scores['Macro Recall']=recall_score(y_test,yp,average='macro')\n",
    "    scores['F1 is_humor=1']=f1_score(y_test,yp)\n",
    "    \n",
    "    print(\"Accuracy\",\"Macro F1\",\"Macro Recall\",\"F1 is_humor=1\",sep='\\t')\n",
    "    print(\"{:.6f}\\t{:.6f}\\t{:.6f}\\t{:.6f}\".format(\n",
    "        scores['Accuracy'],scores['Macro F1'],scores['Macro Recall'],scores['F1 is_humor=1']))\n",
    "                            \n",
    "#     print(\"Recall: \", scores['recall'])\n",
    "#     print(\"F1: \", scores['f1'])\n",
    "#     print(\"Accuracy: \", scores['accuracy'])\n",
    "#     print(\"F1 Humor: \", scores['f1_humor'])\n",
    "    #Vemos la frontera de decisión\n",
    "    plotDecisionBoundary(clf,train, y_train)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un problema de texto  (Identificación de humor HAHA -2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este ejemplo utilizaremos los datos proporcionados para la tarea 1 del HAHA - Humor Analysis based on Human Annotation, la cual consiste determinar si tweets en español son humorísticos o no (un problema de clasificación binaria). Esta tarea forma parte del Iberian Languages Evaluation Forum (IberLEF 2019)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus \n",
    "\n",
    "El conjunto de datos fue etiquetado como se indica [5]. Todos los tweets están etiquetados como humorístico **is_humor=1**  o no humorístico **is_humor=0**. Los datos relevantes para el problema se clasificación ser muestran en el siguiente ejemplo:\n",
    "\n",
    "|Desscripción|Clave|Valor|\n",
    "|:------:|:------:|:-----:|\n",
    "|Tweet |text|Después de la tormenta sale... Tu mamá gritando porque no metiste la ropa.|\n",
    "|Etiqueta de clase|is_humor|1|\n",
    "|Representación vectorial|vec| Vectores de dimensión 300 (FastText preentrenados para español) |\n",
    "|Identificador|id|942079817905770496|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos lod datos.\n",
    "train_data=pd.read_json('data/haha_train_ft_pre.json',lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vemos los primeros 5\n",
    "train_data.head()[['id','text','is_humor','vec']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dimensión de los vectores\n",
    "len(train_data.vec[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los datos de prueba\n",
    "test_data=pd.read_json('data/haha_test_ft_pre.json',lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtenemos la representación vectorial para los conjunto de prueba y entrenamiento\n",
    "train,y_train=np.array([np.array(x) for x in train_data.vec]),np.array(train_data.is_humor)\n",
    "test,y_test=np.array([np.array(x) for x in test_data.vec]),np.array(test_data.is_humor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P={\"FastText\":{\"Accuracy\":0.809050,\"Macro F1\":0.793270,\"Macro Recall\":0.786900,\"F1 is_humor=1\":0.736140}} #Performance\n",
    "\n",
    "# \n",
    "P[\"NC.cos.avg\"]=testClassifier(NearestCentroid(distance='coseno',centroid_type='Average'),train,y_train,test,y_test)\n",
    "P[\"NC.euc.avg\"]=testClassifier(NearestCentroid(distance='euclidiana',centroid_type='Average'),train,y_train,test,y_test)\n",
    "\n",
    "P[\"NC.cos.sum\"]=testClassifier(NearestCentroid(distance='coseno',centroid_type='Sum'),train,y_train,test,y_test)\n",
    "P[\"NC.euc.sum\"]=testClassifier(NearestCentroid(distance='euclidiana',centroid_type='Sum'),train,y_train,test,y_test)\n",
    "\n",
    "P[\"NC.cos.rocchio\"]=testClassifier(NearestCentroid(distance='coseno',centroid_type='Rocchio',beta=16,gamma=4),train,y_train,test,y_test)\n",
    "P[\"NC.euc.rocchio\"]=testClassifier(NearestCentroid(distance='euclidiana',centroid_type='Rocchio',beta=16,gamma=4),train,y_train,test,y_test)\n",
    "\n",
    "P[\"NC.cos.nsum\"]=testClassifier(NearestCentroid(distance='coseno',centroid_type='NormSum'),train,y_train,test,y_test)\n",
    "P[\"NC.euc.nsum\"]=testClassifier(NearestCentroid(distance='euclidiana',centroid_type='NormSum'),train,y_train,test,y_test)\n",
    "\n",
    "\n",
    "P[\"KNN.cos.u\"]=testClassifier(kNN(distance='coseno',k=5,weight_type='uniform'),train,y_train,test,y_test)\n",
    "P[\"KNN.euc.u\"]=testClassifier(kNN(distance='euclidiana',k=5,weight_type='uniform'),train,y_train,test,y_test)\n",
    "\n",
    "P[\"KNN.cos.m\"]=testClassifier(kNN(distance='coseno',k=5,weight_type='mean_dist'),train,y_train,test,y_test)\n",
    "P[\"KNN.euc.m\"]=testClassifier(kNN(distance='euclidiana',k=5,weight_type='mean_dist'),train,y_train,test,y_test)\n",
    "\n",
    "P[\"KNN.cos.w\"]=testClassifier(kNN(distance='coseno',k=5,weight_type='weighed_dist'),train,y_train,test,y_test)\n",
    "P[\"KNN.euc.w\"]=testClassifier(kNN(distance='euclidiana',k=5,weight_type='weighed_dist'),train,y_train,test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolbox import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     240 data/haha_all_ft_pre_min10.json\r\n",
      "    7201 data/haha_test_ft_pre.json\r\n",
      "      72 data/haha_test_ft_pre_min10.json\r\n",
      "   16799 data/haha_train_ft_pre.json\r\n",
      "     168 data/haha_train_ft_pre_min10.json\r\n",
      "   24480 total\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l data/haha_*.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16799\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l data/haha_train_ft_pre.json | cut -d' ' -f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'wc -l data/haha_train_ft_pre.json | cut -d -f1' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-2d876239178e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wc -l data/haha_train_ft_pre.json | cut -d'\u001b[0m \u001b[0;34m' -f1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mcheck_output\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n\u001b[0;32m--> 411\u001b[0;31m                **kwargs).stdout\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m             raise CalledProcessError(retcode, process.args,\n\u001b[0;32m--> 512\u001b[0;31m                                      output=stdout, stderr=stderr)\n\u001b[0m\u001b[1;32m    513\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mCompletedProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'wc -l data/haha_train_ft_pre.json | cut -d -f1' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "result = subprocess.check_output(\"$(wc -l data/haha_train_ft_pre.json | cut -d' ' -f1)\", shell=True)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ipykernel.iostream.OutStream object at 0x7f9b85b1ce90>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(256, 256)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_n=os.system('wc -l haha_train_ft_pre.json')\n",
    "print(sys.stdout)\n",
    "test_n=os.system('wc -l haha_test_ft_pre.json')\n",
    "train_n,test_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l data/haha_*.json\n",
    "# !cat data/haha_train_ft_pre_min10.json > data/haha_all_ft_pre_min10.json\n",
    "# !cat data/haha_test_ft_pre_min10.json >> data/haha_all_ft_pre_min10.json\n",
    "# !wc -l data/haha_all_ft_pre_min10.json\n",
    "!cat data/haha_train_ft_pre.json > data/haha_all_ft_pre.json\n",
    "!cat data/haha_test_ft_pre.json >> data/haha_all_ft_pre.json\n",
    "!wc -l data/haha_all_ft_pre.json\n",
    "\n",
    "!rm -f *.pickle\n",
    "inv_idx = InvertedIdx(\"data/haha_all_ft_pre.json\")\n",
    "inv_idx.process(showProgressEach=10000, stemm=False)\n",
    "inv_idx.compute_mtx()\n",
    "tfidf=pd.DataFrame(inv_idx.idx_mtx.todense(), index=inv_idx.corpus).T\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2=np.ascontiguousarray(tfidf.head(168).to_numpy())\n",
    "test2=np.ascontiguousarray(tfidf.tail(72).to_numpy())\n",
    "train2.shape,test2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "P2={} #Performance\n",
    "\n",
    "# \n",
    "P2[\"NC.cos.avg\"]=testClassifier(NearestCentroid(distance='coseno',centroid_type='Average'),train2,y_train,test2,y_test)\n",
    "P2[\"NC.euc.avg\"]=testClassifier(NearestCentroid(distance='euclidiana',centroid_type='Average'),train2,y_train,test2,y_test)\n",
    "\n",
    "P2[\"NC.cos.sum\"]=testClassifier(NearestCentroid(distance='coseno',centroid_type='Sum'),train2,y_train,test2,y_test)\n",
    "P2[\"NC.euc.sum\"]=testClassifier(NearestCentroid(distance='euclidiana',centroid_type='Sum'),train2,y_train,test2,y_test)\n",
    "\n",
    "P2[\"NC.cos.rocchio\"]=testClassifier(NearestCentroid(distance='coseno',centroid_type='Rocchio',beta=16,gamma=4),train2,y_train,test2,y_test)\n",
    "P2[\"NC.euc.rocchio\"]=testClassifier(NearestCentroid(distance='euclidiana',centroid_type='Rocchio',beta=16,gamma=4),train2,y_train,test2,y_test)\n",
    "\n",
    "P2[\"NC.cos.nsum\"]=testClassifier(NearestCentroid(distance='coseno',centroid_type='NormSum'),train2,y_train,test2,y_test)\n",
    "P2[\"NC.euc.nsum\"]=testClassifier(NearestCentroid(distance='euclidiana',centroid_type='NormSum'),train2,y_train,test2,y_test)\n",
    "\n",
    "\n",
    "P2[\"KNN.cos.u\"]=testClassifier(kNN(distance='coseno',k=5,weight_type='uniform'),train2,y_train,test2,y_test)\n",
    "P2[\"KNN.euc.u\"]=testClassifier(kNN(distance='euclidiana',k=5,weight_type='uniform'),train2,y_train,test2,y_test)\n",
    "\n",
    "P2[\"KNN.cos.m\"]=testClassifier(kNN(distance='coseno',k=5,weight_type='mean_dist'),train2,y_train,test2,y_test)\n",
    "P2[\"KNN.euc.m\"]=testClassifier(kNN(distance='euclidiana',k=5,weight_type='mean_dist'),train2,y_train,test2,y_test)\n",
    "\n",
    "P2[\"KNN.cos.w\"]=testClassifier(kNN(distance='coseno',k=5,weight_type='weighed_dist'),train2,y_train,test2,y_test)\n",
    "P2[\"KNN.euc.w\"]=testClassifier(kNN(distance='euclidiana',k=5,weight_type='weighed_dist'),train2,y_train,test2,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(P).T.sort_values(\"F1 is_humor=1\",ascending =False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.DataFrame(P2).T.sort_values(\"F1 is_humor=1\",ascending =False)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.describe()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
